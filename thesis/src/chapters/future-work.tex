\chapter{Directions for Future Work}

Millipyde is still only the beginning of a new framework. Over the course of its development, it more often raised questions about modeling GPU programming than it did answer existing questions. There are a variety of directions Millipyde can be taken in going forward -- both in regards to enhancing the current functionality and expanding into new functionality. This chapter will discuss some of these potential directions with the hopes that some of them will be tackled in future iterations of the framework.

\section{Parameter Tuning}

By nature, GPU acceleration involves tuning and micro-benchmarking in order to achieve the best results. It often involves finding the right balance of parameters such as grid/block dimensions, shared memory utilization, memory access patterns, and more. For Millipyde, these parameters were chosen based on the result of tests performed on our experimental AMD workstation. The parameters were then hard-coded into the Millipyde code-base. In practice, these values are likely to vary across different machines with different hardware configurations. This is especially true as new generations of GPUs are released. The trade-offs we made today may not necessarily be the same trade-offs that would give us optimal performance in the future.

\quad Ideally, Millipyde should adapt to new environments and tune its parameters accordingly. A basic approach might be to analyze the properties of the devices that are recognized during module initialization. The framework can use these values to make educated guesses about what parameters to use in executing device kernels. A more sophisticated approach could leverage runtime analysis to make decisions and change parameter values once a kernel has completed. For efficiency, it would be important to cache these values so that they do not need to be re-computed each time a Millipyde program is run. It may be possible to do this using the built-in \verb|__pycache__| folder.

\section{Runtime Scheduling Analysis}

Millipyde makes scheduling decisions using two main steps. The first is to pick the best devices available to use. If a problem is not going to use all available devices, it selects from the available devices using their clock frequencies and maximum supported compute units. The second step is to attempt to divide up the task evenly by distributing work to these devices. This means that Millipyde does not take into account any changes that could happen during runtime. In the future, Millipyde should have a way of monitoring the actual metrics of each device while scheduling tasks. If one device is low on available global memory, for example, then the framework should choose other devices for use in allocating new objects. If one device is shown to have better runtime performance than another, Millipyde should factor that in when making scheduling decisions. And ideally, Millipyde should be able to move around tasks and data during runtime to respond to changes as they happen.

\section{Multiprocess Utilization}

Due to the GIL and limitations on Python's ability to use threads, many Python programs instead turn to multiprocessing as a means of parallel acceleration. This is usually done with the \verb|multiprocessing| package that includes objects such as the \verb|Pool| and \verb|Process| for organizing execution as well as objects such as the \verb|Queue| for exchanging data. In the future, it seems natural for Millipyde to support these abstractions of process-based parallelization in its own framework. Ideally, GPU-based tasks can be assigned to different processes, and integration with the existing Python means of inter-process communication can enhance the tools that Millipyde has for exchanging data. This could provide large benefits to any Millipyde users that are already familiar with Python's model of processing. 

\section{Multi-node Utilization}

Another layer of abstraction that Millipyde could exploit for asynchronous execution is multiple nodes in a computing cluster. As it is today, Millipyde is only able to recognize devices connected to the current system and perform intra-node levels of data transfer. Although this is fine for many problems, GPUs are extremely prevalent in multi-node computing clusters so this remains a huge area for potential future expansion. Although it would likely require large changes to the Millipyde backend, it would allow Millipyde users to tackle larger problems than would otherwise be supported. Since Millipyde is based on the ROCm ecosystem, it would be natural to experiment with ROCm's current tools for inter-node communication. These include the Unified Communication X (UCX) library, and the OpenMPI message passing specification. This would open up many new questions about how work can be scheduled and how systems can be analyzed both during module initialization and during runtime to intelligently distribute work to available nodes and devices. 

\section{Support for User-Defined Kernels}

Currently, all Millipyde GPU kernels are built into the back-end source code itself as pre-defined functions with Python interface wrappers. To make Millipyde as flexible as possible, it should one day cater to the needs of developers who want to write their own GPU-accelerated functions. Many libraries such as CuPy, PyCUDA, and PyOpenCL have experimented with approaches involving Python strings containing GPU kernels written in CUDA C++ syntax that can be then compiled and used in the Python runtime. NVIDIA has taken a similar approach with Python CUDA initiatives announced in 2021 \cite{cudaPython}. Many opportunities still exist for finding new ways of allowing Python users to define their own GPU-accelerated functions. Ideally, future solutions would not require Python users to have to know how to write C++ syntax for kernels, and they should carry on Millipyde's goal of cross-platform flexibility. How these functions can be created and designed from within Python remains an open problem for future work.

\section{Integration with other libraries}

Python libraries seldom exist in a vacuum. One of the great things about Python is how libraries such as NumPy can become the foundation to many new libraries and frameworks that tackle difficult computing problems. Millipyde aims to join these communities and open ecosystems by providing an inter-operable tool for GPU computing. Going forward, more work can be done with exploring interactions between Millipyde and other frameworks and libraries. CuPy, for example, contains many GPU-accelerated functions and tools that can benefit Millipyde's own framework offerings. Another library, Dask, provides NumPy compatible data analysis, data scheduling, and compute-cluster capabilities. These features could be combined with Millipyde's own computing constructs to provide better scheduling and analytics for GPU-based tasks. And these libraries are just the tip of the iceberg. There are many more exciting possibilities for where Millipyde can fit into the ever-growing landscape of Python computing.