\chapter{Related Works}

\section{NumPy}

NumPy is an open source library that has become fundamental to scientific computing in Python. Its main contribution to the language is the \verb|ndarray| object representing a multi-dimensional array that can contain a variety of supported data-types. Since NumPy was developed in C, NumPy contains a range of useful C API tools that allow for other extension modules to take advantage of its functionality \cite{numpyCApi}. Today, NumPy is the backbone for countless scientific and mathematical libraries that build on \verb|ndarray|s and NumPy functionality. 

\quad One of the biggest draws for NumPy is its speed and efficiency. Without NumPy, Python programmers are limited to the built-in Python list which grows dynamically and can support content of mixed data types. NumPy arrays on the other hand are able to achieve better performance through constraints that allow them to behave like arrays in languages like C. NumPy's \verb|ndarray| objects can only contain a consistent data type, and arrays must maintain a fixed shape and size \cite{arrayNumpy}. Using these constraints, operations can be optimized and parallelized to efficiently act on the sequence of data. NumPy uses the idea of vectorization for its operations which are built on pre-compiled C code \cite{numpyDocs}. These vectorized functions ``broadcast'' operations across the entire sequence in a notation that is similar to what is seen in mathematics \cite{numpyDocs}. This improves readability by removing the need for explicit loops and iteration within Python.

\section{SciPy and Scikits}

SciPy is a library that has become the standard for modeling and computing scientific problems in Python. SciPy is built on top of NumPy and the \verb|ndarray|, but it adds a variety of new data structures such as sparse matrices and k-dimensional trees \cite{scipy}. These, in combination with methods for manipulating and visualizing data, create an ecosystem for solving a wide variety of problems. Today, more than 100,000 different code repositories use SciPy as a dependency \cite{scipy}. For specialized tasks, SciPy introduced the concept of SciPy toolkits, or SciKits, that add on packages for SciPy. Each SciKit brings specialized functionality for a variety of use-cases. Among these are scikit-image for image processing and scikit-learn which has become the gold-standard for machine learning in Python \cite{scikits}.

\section{Numba}

Numba is a just-in-time (JIT) compiler for CPython that is written as an extension library so that the interpreter does not need to be modified or replaced. Numba specializes in accelerating functions that use \verb|ndarray|s, NumPy functions and operators, and loops. It includes function decorators such as \verb|@jit| that can be dropped in to existing code to make it easy to accelerate without needing to be rewritten. Internally, Numba uses the LLVM compiler infrastructure. Python bytecode is analyzed and turned into an intermediate representation called the Numba IR \cite{lessonsLearned}. From there, Numba will attempt to infer types and lower the code to LLVM so that it can be compiled into efficient machine code. This compilation happens during the runtime of an application when the decorated function is called, but it only has to happen once for each function written since the compiled version is stored in a cache. Because this code is compiled and does not incur the overhead of the Python interpreter, Numba is able to achieve speeds comparable to those of C programs.  

\quad Numba acceleration is not just limited to the CPU. Numba is able to take advantage of GPU acceleration by leveraging CUDA's version of the LLVM library, NVVM. Numba is even able to support ROCm-compatible GPUs by compiling code into HSA kernels and device functions following the HSA execution model \cite{numbaRocm}. Numba is not perfect, however. Due to Numba's high level of abstraction, details and tuning are lost in translation and Numba lacks support for features such as dynamic parallelism and texture memory \cite{gpucomppy}. Python applications using Numba usually reach between 50\% and 85\% performance of the equivalent C/C++ CUDA implementations on compute heavy workloads. \cite{lessonsLearned}.

\section{CuPy}

CuPy is an open source Python library that aims to bring GPU acceleration to NumPy and SciPy. CuPy includes a wide variety of accelerated functions including linear algebra operations, sorting, sparse matrices, and more. In many ways, CuPy can function as a drop-in replacement for NumPy by providing functions of the same names. Originally, CuPy was designed to support CUDA-capable GPUs and leveraged popular CUDA-accelerated libraries such as cuBLAS, cuDNN, cuRAND, cuSOLVER, and cuSPARSE. Now, CuPy has expanded into experimental support for the ROCm ecosystem, and it and uses the equivalent ROCm-based computing libraries \cite{cupyRocm}.

\quad One of CuPy's most versatile features is the ability for users to create their own GPU kernels. Code snippets can be created in C++ syntax that are compiled into binaries that can be cached and used in subsequent runs. User defined kernels have the flexibility to either perform element-wise operations on all items in the data, or reduction kernels to fold data using binary operators \cite{cupy}. This allows for powerful flexibility and more control over GPU execution from Python at the expense of requiring the programmer to know kernel code design in C++ syntax. Even using the built-in functions, however, CuPy programmers can see tens to 100s of times speedup over NumPy when playing to CuPy's strengths.

\section{PyCuda and PyOpenCL}

PyCUDA and PyOpenCL are two open source toolkits built on CUDA and OpenCL respectively. They take the approach of using runtime code generation (RTCG) in order to take advantage of GPU parallelism from within Python code. RTCG works off of the idea of ``metaprogramming'' where code is tasked with interpreting and creating different sets of code to solve a problem \cite{pycuda}. In the case of PyCUDA, CUDA kernels can be written as strings within the Python code that are compiled at runtime using the NVCC compiler and run on the GPU as a binary. The compiled GPU code can be cached for future re-use which increases the efficiency of the application on subsequent runs. By extending support to OpenCL, PyOpenCL is able to work across a variety of platforms independent of the GPU manufacturer. 

\quad PyCUDA and PyOpenCL use a variety of techniques to tune the generated code. Loop slicing helps preserve locality of data access and use cache memory efficiently. The code can also can also adapt based on the amount of on-chip user-managed memory as well. Finally, tuning is performed based on available DRAM bandwidth. Although PyCUDA and PyOpenCL reduce the overhead of writing applications in pure C, C++, and CUDA, the programmer still needs to be familiar with the creation of GPU kernels in order to write the code templates within the Python program. This is not unlike kernel generation mechanisms in other libraries such as CuPy.

\section{TensorFlow}

TensorFlow is one of the most prevalent libraries for developing and training machine learning models, specializing in deep neural networks. It is compatible with a variety of architectures allowing TensorFlow to be deployed on CPU, GPU and TPU-based environments \cite{tensorflow}. It is also extremely flexible when it comes to the scale of the system it is deployed on. TensorFlow can run on devices as small as mobile phones or embedded devices, and it can scale up to run on large scale distributed systems with thousands of compute devices \cite{largeScaleTensorflow}. This makes it a powerful tool for a variety of machine learning fields such as speech recognition, natural language processing, computer vision, and more. TensorFlow also fits in very well with Python's library ecosystem. It is built around a multi-dimensional array type called a tensor, which are comparable to NumPy's \verb|ndarray|s, and these tensors are even compatible with NumPy operations. 


\section{Keras}

Keras is a high-level API for neural networks that is built on top of TensorFlow and other supported backends. Its interface is designed to be as simple and flexible as possible, allowing its users to quickly prototype and create deep learning models. Keras is built around two data structures called models and layers. At its most simple level, a sequential stack of layers are combined together to create a sequential model. More complicated models are built around arbitrary graph topologies that can handle a wide variety of machine learning problems \cite{keras}. 